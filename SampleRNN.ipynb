{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/ktonal/mimikit@experiment/new-data-again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to gdrive for making a db with your audios (a `freqnet-db` won't work...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/gdrive')\n",
    "%cd /gdrive/MyDrive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.lib.stride_tricks import as_strided as np_as_strided\n",
    "from pytorch_lightning import LightningModule\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Sampler, RandomSampler, BatchSampler\n",
    "import math\n",
    "from typing import Optional\n",
    "from dataclasses import dataclass\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from mimikit.kit.connectors.neptune import NeptuneConnector\n",
    "from mimikit.utils import audio\n",
    "from mimikit.audios import transforms as A\n",
    "from mimikit.kit.ds_utils import ShiftedSequences\n",
    "from mimikit.kit import DBDataset, SuperAdam, SequenceModel, DataSubModule\n",
    "from mimikit.kit import get_trainer\n",
    "from mimikit.kit import tqdm\n",
    "\n",
    "\n",
    "# first we define a sampler & a db then the network. Finaly, we \"merge\" them all in a LightningModule.\n",
    "\n",
    "class TBPTTSampler(Sampler):\n",
    "    \"\"\"\n",
    "    yields batches of indices for performing Truncated Back Propagation Through Time\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 n_samples,\n",
    "                 batch_size=64,  # nbr of \"tracks\" per batch\n",
    "                 chunk_length=8*16000,  # total length of a track\n",
    "                 seq_len=1024  # nbr of samples per backward pass\n",
    "                 ):\n",
    "        super().__init__(None)\n",
    "        self.n_samples = n_samples\n",
    "        self.batch_size = batch_size\n",
    "        self.chunk_length = chunk_length\n",
    "        self.seq_len = seq_len\n",
    "        self.n_chunks = self.n_samples // self.chunk_length\n",
    "        self.n_per_chunk = self.chunk_length // self.seq_len\n",
    "\n",
    "    def __iter__(self):\n",
    "        smp = RandomSampler(torch.arange(self.n_chunks))\n",
    "        for top in BatchSampler(smp, self.batch_size, True):  # drop last!\n",
    "            for start in range(self.n_per_chunk):\n",
    "                # start indices of the batch\n",
    "                yield tuple((t * self.chunk_length) + (start * self.seq_len) for t in top)\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(max(1, math.floor(self.n_chunks / self.batch_size)) * self.n_per_chunk)\n",
    "\n",
    "\n",
    "class FramesDB(DBDataset):\n",
    "    qx = None\n",
    "\n",
    "    @staticmethod\n",
    "    def extract(path, sr=16000, mu=255):\n",
    "        signal = A.FileTo.mu_law_compress(path, sr=sr, mu=mu)\n",
    "        return dict(qx=(dict(sr=sr, mu=mu), signal.reshape(-1, 1), None))\n",
    "\n",
    "    def prepare_dataset(self, model, datamodule):\n",
    "        batch_size, chunk_len, batch_seq_len, frame_sizes = model.batch_info()\n",
    "        shifts = [frame_sizes[0] - size for size in frame_sizes + (0,)]  # (0,) for the target\n",
    "        lengths = [batch_seq_len for _ in frame_sizes[:-1]]\n",
    "        # bottom tier\n",
    "        lengths += [frame_sizes[0] + batch_seq_len]\n",
    "        # targets\n",
    "        lengths += [batch_seq_len]\n",
    "\n",
    "        self.slicer = ShiftedSequences(len(self.qx), list(zip(shifts, lengths)))\n",
    "        self.frame_sizes = frame_sizes\n",
    "        self.seq_len = batch_seq_len\n",
    "\n",
    "        # round the size of the dataset to a multiple of the chunk size :\n",
    "        batch_sampler = TBPTTSampler(chunk_len * (len(self.qx) // chunk_len),\n",
    "                                     batch_size,\n",
    "                                     chunk_len,\n",
    "                                     batch_seq_len)\n",
    "        datamodule.loader_kwargs.update(dict(batch_sampler=batch_sampler))\n",
    "        for k in [\"batch_size\", \"shuffle\", \"drop_last\"]:\n",
    "            if k in datamodule.loader_kwargs:\n",
    "                datamodule.loader_kwargs.pop(k)\n",
    "        datamodule.loader_kwargs[\"sampler\"] = None\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        if type(self.qx) is not torch.Tensor:\n",
    "            itemsize = self.qx.dtype.itemsize\n",
    "            as_strided = lambda slc, fs: np_as_strided(self.qx[slc],\n",
    "                                                       shape=(self.seq_len, fs),\n",
    "                                                       strides=(itemsize, itemsize))\n",
    "        else:\n",
    "            as_strided = lambda slc, fs: torch.as_strided(self.qx[slc],\n",
    "                                                          size=(self.seq_len, fs),\n",
    "                                                          stride=(1, 1))\n",
    "\n",
    "        slices = self.slicer(item)\n",
    "        tiers_slc, bottom_slc, target_slc = slices[:-2], slices[-2], slices[-1]\n",
    "        inputs = [self.qx[slc].reshape(-1, fs) for slc, fs in zip(tiers_slc, self.frame_sizes[:-1])]\n",
    "        # ugly but necessary if self.qx became a tensor...\n",
    "        with torch.no_grad():\n",
    "            inputs += [as_strided(bottom_slc, self.frame_sizes[-1])]\n",
    "\n",
    "        target = self.qx[target_slc]\n",
    "\n",
    "        return tuple(inputs), target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.qx)\n",
    "\n",
    "\n",
    "@dataclass(init=True, repr=False, eq=False, frozen=False, unsafe_hash=True)\n",
    "class SampleRNNTier(nn.Module):\n",
    "    tier_index: int\n",
    "    frame_size: int\n",
    "    dim: int\n",
    "    up_sampling: int = 1\n",
    "    n_rnn: int = 2\n",
    "    q_levels: int = 256\n",
    "    embedding_dim: Optional[int] = None\n",
    "    mlp_dim: Optional[int] = None\n",
    "\n",
    "    is_bottom = property(lambda self: self.up_sampling == 1)\n",
    "\n",
    "    def linearize(self, q_samples):\n",
    "        \"\"\" maps input samples (0 <= qx < 256) to floats (-.5 <= x < .5) \"\"\"\n",
    "        return (q_samples.float() / self.q_levels) - .5\n",
    "\n",
    "    def embeddings_(self):\n",
    "        if self.embedding_dim is not None:\n",
    "            return nn.Embedding(self.q_levels, self.embedding_dim)\n",
    "        return None\n",
    "\n",
    "    def input_proj_(self):\n",
    "\n",
    "        if not self.is_bottom:  # top & middle tiers\n",
    "            return nn.Linear(self.frame_size, self.dim, bias=False)\n",
    "\n",
    "        else:  # bottom tier\n",
    "            class BottomProjector(nn.Module):\n",
    "                def __init__(self, emb_dim, out_dim, frame_size):\n",
    "                    super(BottomProjector, self).__init__()\n",
    "                    self.cnv = nn.Conv1d(emb_dim, out_dim, kernel_size=frame_size, bias=False)\n",
    "\n",
    "                def forward(self, hx):\n",
    "                    \"\"\" hx : (B x T x FS x E) \"\"\"\n",
    "                    B, T, FS, E = hx.size()\n",
    "                    hx = self.cnv(hx.view(B * T, FS, E).transpose(1, 2).contiguous())\n",
    "                    # now hx : (B*T, DIM, 1)\n",
    "                    return hx.squeeze().reshape(B, T, -1)\n",
    "\n",
    "            return BottomProjector(self.embedding_dim, self.dim, self.frame_size)\n",
    "\n",
    "    def rnn_(self):\n",
    "        # no rnn for bottom tier\n",
    "        if self.is_bottom:\n",
    "            return None\n",
    "\n",
    "        return nn.LSTM(self.dim, self.dim, self.n_rnn, batch_first=True)\n",
    "\n",
    "    def up_sampling_net_(self):\n",
    "        # no up sampling for bottom tier\n",
    "        if self.is_bottom:\n",
    "            return None\n",
    "\n",
    "        class TimeUpscalerLinear(nn.Module):\n",
    "\n",
    "            def __init__(self, in_dim, out_dim, up_sampling, **kwargs):\n",
    "                super(TimeUpscalerLinear, self).__init__()\n",
    "                self.up_sampling = up_sampling\n",
    "                self.out_dim = out_dim\n",
    "                self.fc = nn.Linear(in_dim, out_dim * up_sampling, **kwargs)\n",
    "\n",
    "            def forward(self, x):\n",
    "                B, T, _ = x.size()\n",
    "                return self.fc(x).reshape(B, T * self.up_sampling, self.out_dim)\n",
    "\n",
    "        return TimeUpscalerLinear(self.dim, self.dim, self.up_sampling)\n",
    "\n",
    "    def mlp_(self):\n",
    "        if not self.is_bottom:\n",
    "            return None\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(self.dim, self.mlp_dim), nn.ReLU(),\n",
    "            nn.Linear(self.mlp_dim, self.mlp_dim), nn.ReLU(),\n",
    "            nn.Linear(self.mlp_dim, self.q_levels),\n",
    "        )\n",
    "\n",
    "    def __post_init__(self):\n",
    "        nn.Module.__init__(self)\n",
    "        self.embeddings = self.embeddings_()\n",
    "        self.inpt_proj = self.input_proj_()\n",
    "        self.rnn = self.rnn_()\n",
    "        self.up_net = self.up_sampling_net_()\n",
    "        self.mlp = self.mlp_()\n",
    "\n",
    "    def forward(self, input_samples, prev_tier_output=None, hidden=None):\n",
    "\n",
    "        if self.embeddings is None:\n",
    "            x = self.linearize(input_samples)\n",
    "        else:\n",
    "            x = self.embeddings(input_samples)\n",
    "\n",
    "        if self.inpt_proj is not None:\n",
    "            p = self.inpt_proj(x)\n",
    "            if prev_tier_output is not None:\n",
    "                x = p + prev_tier_output\n",
    "            else:\n",
    "                x = p\n",
    "\n",
    "        if self.rnn is not None:\n",
    "            if hidden is None or x.size(0) != hidden[0].size(1):\n",
    "                h0 = (torch.randn(self.n_rnn, x.size(0), self.dim) * .05).to(x)\n",
    "                c0 = (torch.randn(self.n_rnn, x.size(0), self.dim) * .05).to(x)\n",
    "                hidden = (h0, c0)\n",
    "            else:\n",
    "                # TRUNCATED back propagation through time == detach()!\n",
    "                hidden = tuple(h.detach() for h in hidden)\n",
    "\n",
    "            x, hidden = self.rnn(x, hidden)\n",
    "\n",
    "        if self.up_net is not None:\n",
    "            x = self.up_net(x)\n",
    "\n",
    "        if self.mlp is not None:\n",
    "            x = self.mlp(x)\n",
    "\n",
    "        return x, hidden\n",
    "\n",
    "\n",
    "@dataclass(init=True, repr=False, eq=False, frozen=False, unsafe_hash=True)\n",
    "class SampleRNNNetwork(nn.Module):\n",
    "\n",
    "    frame_sizes: tuple  # from top to bottom!\n",
    "    dim: int = 512\n",
    "    n_rnn: int = 2\n",
    "    q_levels: int = 256\n",
    "    embedding_dim: int = 256\n",
    "    mlp_dim: int = 512\n",
    "\n",
    "    def __post_init__(self):\n",
    "        nn.Module.__init__(self)\n",
    "        n_tiers = len(self.frame_sizes)\n",
    "        tiers = []\n",
    "        if n_tiers > 2:\n",
    "            for i, fs in enumerate(self.frame_sizes[:-2]):\n",
    "                tiers += [SampleRNNTier(i, fs, self.dim,\n",
    "                                        up_sampling=fs // self.frame_sizes[i + 1],\n",
    "                                        n_rnn=self.n_rnn,\n",
    "                                        q_levels=self.q_levels,\n",
    "                                        embedding_dim=None,\n",
    "                                        mlp_dim=None)]\n",
    "        # before last tier\n",
    "        tiers += [SampleRNNTier(n_tiers - 2, self.frame_sizes[-2], self.dim,\n",
    "                                up_sampling=self.frame_sizes[-1],\n",
    "                                n_rnn=self.n_rnn,\n",
    "                                q_levels=self.q_levels,\n",
    "                                embedding_dim=None,\n",
    "                                mlp_dim=None)]\n",
    "        # bottom tier\n",
    "        tiers += [SampleRNNTier(n_tiers - 1, self.frame_sizes[-1], self.dim,\n",
    "                                up_sampling=1,\n",
    "                                n_rnn=self.n_rnn,\n",
    "                                q_levels=self.q_levels,\n",
    "                                embedding_dim=self.embedding_dim,\n",
    "                                mlp_dim=self.mlp_dim)]\n",
    "\n",
    "        self.tiers = nn.ModuleList(tiers)\n",
    "        self.hidden = [None] * n_tiers\n",
    "\n",
    "    def forward(self, tiers_inputs):\n",
    "        prev_out = None\n",
    "        for i, (tier, inpt) in enumerate(zip(self.tiers, tiers_inputs)):\n",
    "            prev_out, self.hidden[i] = tier(inpt, prev_out, self.hidden[i])\n",
    "        return prev_out\n",
    "\n",
    "    def reset_h0(self):\n",
    "        self.hidden = [None] * len(self.frame_sizes)\n",
    "\n",
    "\n",
    "class SampleRNN(SequenceModel,\n",
    "                DataSubModule,\n",
    "                SuperAdam,\n",
    "                SampleRNNNetwork,\n",
    "                LightningModule):\n",
    "\n",
    "    @staticmethod\n",
    "    def loss_fn(output, target):\n",
    "        criterion = nn.CrossEntropyLoss(reduction=\"mean\")\n",
    "        return criterion(output.view(-1, output.size(-1)), target.view(-1))\n",
    "\n",
    "    db_class = FramesDB\n",
    "\n",
    "    def __init__(self,\n",
    "                 frame_sizes=(4, 4),\n",
    "                 net_dim=512,\n",
    "                 emb_dim=256,\n",
    "                 mlp_dim=512,\n",
    "                 n_rnn=2,\n",
    "                 q_levels=256,  # == mu + 1\n",
    "                 max_lr=1e-3,\n",
    "                 betas=(.9, .9),\n",
    "                 div_factor=3.,\n",
    "                 final_div_factor=1.,\n",
    "                 pct_start=.25,\n",
    "                 cycle_momentum=True,\n",
    "                 db=None,\n",
    "                 files=None,\n",
    "                 batch_size=64,\n",
    "                 batch_seq_len=1024,\n",
    "                 chunk_len=8*16000,\n",
    "                 in_mem_data=True,\n",
    "                 splits=None,  # tbptt should implement the splits...\n",
    "                 **loaders_kwargs\n",
    "                 ):\n",
    "        super(LightningModule, self).__init__()\n",
    "        SequenceModel.__init__(self)\n",
    "        DataSubModule.__init__(self, db, files, in_mem_data, splits, batch_size=batch_size, **loaders_kwargs)\n",
    "        SuperAdam.__init__(self, max_lr, betas, div_factor, final_div_factor, pct_start, cycle_momentum)\n",
    "        SampleRNNNetwork.__init__(self, frame_sizes, net_dim, n_rnn, q_levels, emb_dim, mlp_dim)\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def batch_info(self, *args, **kwargs):\n",
    "        return tuple(self.hparams[key] for key in [\"batch_size\", \"chunk_len\", \"batch_seq_len\", \"frame_sizes\"])\n",
    "\n",
    "    def setup(self, stage: str):\n",
    "        SuperAdam.setup(self, stage)\n",
    "\n",
    "    def on_train_batch_start(self, batch, batch_idx, dataloader_idx):\n",
    "        if (batch_idx * self.hparams.batch_seq_len) % self.hparams.chunk_len == 0:\n",
    "            self.reset_h0()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make or Get a db\n",
    "\n",
    "Supposed to throw an error! Uncomment the lines you need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_name = \"test_rnn.h5\"\n",
    "\n",
    "nep_con = NeptuneConnector(user=\"k-tonal\",\n",
    "                           setup={\n",
    "                               \"db\": \"data-and-base-notebooks\",\n",
    "                                 })\n",
    "\n",
    "\n",
    "# use this 2 lines to create a db and upload it :\n",
    "\n",
    "# db = SampleRNN.db_class.make(db_name, roots=[\"./\"], sr=16000, mu=255)\n",
    "# nep_con.upload_database(\"db\", db_name)\n",
    "\n",
    "# use those 2 lines to download and open a db that exists\n",
    "\n",
    "# nep_con.download_database(\"db\")\n",
    "# db = SampleRNN.db_class(db_name)\n",
    "\n",
    "db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About the model's args\n",
    "\n",
    "\n",
    "- the [original paper](https://arxiv.org/pdf/1612.07837.pdf) has some recommendations, and the [Dadabots](http://dadabots.com/nips2017/generating-black-metal-and-math-rock.pdf) made a pretty good job at describing their experiments.\n",
    "\n",
    "\n",
    "- the most important arg is `frame_sizes` (see [the original repo](https://github.com/soroushmehr/sampleRNN_ICLR2017) for a visual aid to this)\n",
    "\n",
    "    - `SampleRNN` doesn't have \"layers\", it has \"tiers\" : small models that process `frame_size` inputs at a time and combine their outputs with the outputs of the previous tier.\n",
    "    \n",
    "    - `frame_sizes` argument determine how many samples each tier (from top to bottom!) processes at a time. The repo's image corresponds to `frame_sizes=(16, 4, 4)` for tier 3, 2, 1, which does, in fact work pretty well...\n",
    "    \n",
    "    - **IMPORTANT!** you can have as many tiers as you want, but :\n",
    "    \n",
    "        1. the two last tiers must have the same `frame_size`\n",
    "        \n",
    "        2. dividing a tier's frame_size by the next tier's frame_size should always result in an exact integer. e.g. \n",
    "            - (128, 4, 4) => **yes** because 128 / 4 == 32\n",
    "            - (12, 11, 11) => **no** because 12 / 11 == 1.0909090909090908\n",
    "            \n",
    "        3. The first frame_size has to be smaller or equal to an other arg : `batch_seq_length`\n",
    "\n",
    "    - the original paper says `(8, 2, 2)` worked best. Dadabots used only 2 tiers, probably `(4, 4)` or similar. With this implementation you could go wild and do `(256, 128, 64, 32, 16, 8, 4, 2, 2)` or even more...\n",
    "    \n",
    "\n",
    "- Each tier but the last has a Recurrent Network with 1 or more layers. The `n_rnn` argument specifies how many layers **per tier**. It seems to me that it starts working when the whole model has a total of at least 4 rnns : e.g. `frame_sizes=(8, 2, 2)` & `n_rnn=2` corresponds to 4 rnns total (last tier always has 0 rnns). Dadabots made their streams with 2 tiers and the top tier had between 5 and 9 rnns...\n",
    "\n",
    "\n",
    "- `*_dim` arguments are very similar to `model_dim` in `FreqNet`. \n",
    "\n",
    "    - `net_dim` is the most important and will greatly influence the trade-off between speed & expressivity. It could have been named `model_dim` because most of the network's parameters will have sizes proportional to `net_dim`. `512` works well. Maybe you can go down to `256` for more speed or up to `1024` for more expressivity... Definitely worths playing with!\n",
    "    \n",
    "    - `emb_dim` is just for a few parameters and might not be very important. `256` works, but I would expect so would `128` or `64`, maybe even `32`... More than `256` could be too much but, honnestly, IDUNO!... :)\n",
    "    \n",
    "    - `mlp_dim` is for the tipp of the model (which makes the prediction). `512` works. Once again, I'm not sure how relevant this `dim` is...\n",
    "\n",
    "\n",
    "- `max_epochs` : it seems SampleRNN generates quite well very early! Values for the loss that resulted in cool outputs for me were around 1.6 to 1.9 and this was after just a few epochs! It seems even that training too long results in long silent outputs, this happened to me after 100 epochs and a loss around 1.4.\n",
    "\n",
    "\n",
    "- `max_lr` : it also seems that SampleRNN withstands high learning rates, which also means faster training! As a comparaison, freqnet starts to diverge with `max_lr > 1e-3` but here `5e-3` works, even if it's probably already at the limit... If the loss starts to increase, fall back to `max_lr=1e-3` and you should be fine.\n",
    "    \n",
    "    \n",
    "- the values used in the next cell seem to work quite well. In doubt, use them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = SampleRNN(db=db, \n",
    "                net_dim=512,\n",
    "                emb_dim=256,\n",
    "                mlp_dim=512,\n",
    "                n_rnn=2,\n",
    "                frame_sizes=(16, 4, 4),\n",
    "                max_lr=5e-3,\n",
    "                div_factor=3.,\n",
    "                betas=(.9, .9),\n",
    "                batch_size=64,\n",
    "                batch_seq_len=1024\n",
    "               )\n",
    "\n",
    "trainer = get_trainer(root_dir=\"test_rnn\",\n",
    "                      max_epochs=50,\n",
    "                      epochs=[49],\n",
    "                     # uncomment these if you want to track with neptune :\n",
    "#                      model=net,\n",
    "#                      neptune_connector=nep_con,\n",
    "                     )\n",
    "\n",
    "net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train\n",
    "you can stop the training at any time and start/resume it with the next cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation\n",
    "\n",
    "Generating with SampleRNN is quite flexible!\n",
    "\n",
    "1. The model has an internal state that is suppose to encode what the model has seen _until now_. So before we let him run loose, we can \"warm up\" the model with a prompt of `n_warmups` batches. (I'm not sure if it changes much for the outputs but it's worth playing with...)\n",
    "\n",
    "\n",
    "2. The generation method has 2 modes : deterministic and probabilistic. In the first, the output will always be the same for the same prompt/warm-up. But the second mode samples the outputs from probabilities that can be modified with a very interesting parameter : `temperature`.\n",
    "    - `temperature` must be bigger than 0. and altough it could theoretically be greater than 1., values above 1. might not be so interesting because :\n",
    "    - the higher the `temperature`, the \"noisier\" the output. The lower the temp, the more \"frozen\" the output. It is called \"temperature\" because it corresponds to some heat equations : more heat = particles move faster, less heat = particles stop moving. Musically, it means : hotter = more contrasts, cooler = longer sounds.\n",
    "    - Concretly, I recommend starting around `temperature=0.5` and going tiny bits up & down....\n",
    "    \n",
    "    \n",
    "3. Because generating in time-domain is much slower than in freq-dom, **generation is split in 2 cells**:\n",
    "    - the first gets a **new prompt** and do some warmups\n",
    "    - the second generates and **appends** the results to what has been previously generated.\n",
    "This way, you can evaluate the first once and evaluate the 2nd several times. This beats waiting 30min to discover that it generated 30 seconds of silence...\n",
    "\n",
    "\n",
    "4. You can generate `n_prompts` at the same time (like in redundance rate). This is much faster than generating one prompt at a time.\n",
    "\n",
    "\n",
    "5. Because feeding data to SampleRNN is a bit complex, you'll have to stick to random prompts from the training data for now..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WARM-UP\n",
    "\n",
    "n_prompts = 8\n",
    "n_warmups = 20\n",
    "\n",
    "\n",
    "net.eval()\n",
    "net = net.to(\"cuda\")\n",
    "net.reset_h0()\n",
    "\n",
    "dl = iter(net.datamodule.train_dataloader())\n",
    "for _ in tqdm(range(n_warmups)):\n",
    "    inpt, trgt = next(dl)\n",
    "    inpt = tuple(x[:n_prompts].to(\"cuda\") for x in inpt)\n",
    "    with torch.no_grad():\n",
    "        new = nn.Softmax(dim=-1)(net(inpt)).argmax(dim=-1)\n",
    "        \n",
    "new.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### PLAY WITH THOSE : ###################\n",
    "\n",
    "# to use the deterministic mode, set temperature to None\n",
    "\n",
    "temperature = 0.5\n",
    "\n",
    "# 1 second = 16000 steps !\n",
    "\n",
    "n_steps = 32000\n",
    "\n",
    "###############################################\n",
    "\n",
    "## LOS GEHT'S!\n",
    "\n",
    "fs = [*net.frame_sizes]\n",
    "outputs = [None] * (len(fs) - 1)\n",
    "hiddens = net.hidden\n",
    "tiers = net.tiers\n",
    "\n",
    "with torch.no_grad():\n",
    "    for t in tqdm(range(new.size(1), n_steps + new.size(1))):\n",
    "        for i in range(len(tiers)-1):\n",
    "            if t % fs[i] == 0:\n",
    "                inpt = new[:, t-fs[i]:t].reshape(-1, 1, fs[i])\n",
    "                \n",
    "                if i == 0:\n",
    "                    prev_out = None\n",
    "                else:\n",
    "                    prev_out = outputs[i-1][:, (t // fs[i]) % (fs[i-1] // fs[i])]\n",
    "                    \n",
    "                out, h = tiers[i](inpt, prev_out, hiddens[i])\n",
    "                hiddens[i] = h\n",
    "                outputs[i] = out\n",
    "                \n",
    "        prev_out = outputs[-1]\n",
    "        inpt = new[:, t-fs[-1]:t].reshape(-1, 1, fs[-1])\n",
    "\n",
    "        out, _ = tiers[-1](inpt, prev_out[:, (t % fs[-1]) - fs[-1]].unsqueeze(1))\n",
    "        \n",
    "        if temperature is None:\n",
    "            \n",
    "            pred = (nn.Softmax(dim=-1)(out)).argmax(dim=-1)\n",
    "        \n",
    "        else:\n",
    "        \n",
    "        # great place to implement dynamic cooling/heating !\n",
    "            tempered = out / temperature\n",
    "            \n",
    "            pred = torch.multinomial(nn.Softmax(dim=-1)(tempered).squeeze(), 1)\n",
    "        \n",
    "        new = torch.cat((new, pred), dim=-1)\n",
    "\n",
    "\n",
    "for i in range(new.size(0)):\n",
    "\n",
    "    y = new[i].squeeze().cpu().numpy()\n",
    "    y = A.SignalFrom.mu_law_compressed(y - 128, 255)\n",
    "\n",
    "    print(\"prompt number\", i)\n",
    "    plt.figure(figsize=(20, 8))\n",
    "    plt.plot(y)\n",
    "    plt.show()\n",
    "\n",
    "    audio(y, sr=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
